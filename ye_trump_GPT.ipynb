{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "559608c2edf74a01a89edcaa5ddc8e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc60e8ff4e73432ea33f571f5ed00033",
              "IPY_MODEL_fd86a3f633a742b4aedf814f92f040f3",
              "IPY_MODEL_a7d0084efca64c8f8d5d6231a242277b"
            ],
            "layout": "IPY_MODEL_de3da74f1414449791c2ea6f6a243b3c"
          }
        },
        "cc60e8ff4e73432ea33f571f5ed00033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8d28c9106e04a2caa551f0eb84299b1",
            "placeholder": "​",
            "style": "IPY_MODEL_782c3598ff4c4b3ba92c7c740693c714",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fd86a3f633a742b4aedf814f92f040f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e00652fd8a784f2796256085e46fd518",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c4893fabd974809a197b422d77900d9",
            "value": 26
          }
        },
        "a7d0084efca64c8f8d5d6231a242277b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6df83872a57a4d83b14a4fd9b261f599",
            "placeholder": "​",
            "style": "IPY_MODEL_7be424b000e94e459a9e79dfb36c120c",
            "value": " 26.0/26.0 [00:00&lt;00:00, 964B/s]"
          }
        },
        "de3da74f1414449791c2ea6f6a243b3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d28c9106e04a2caa551f0eb84299b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "782c3598ff4c4b3ba92c7c740693c714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e00652fd8a784f2796256085e46fd518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c4893fabd974809a197b422d77900d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6df83872a57a4d83b14a4fd9b261f599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7be424b000e94e459a9e79dfb36c120c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7526cbbf6e2947e7ad4726cb4d9c6dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1c950a0a5ca4df4900c39d05db9b7dc",
              "IPY_MODEL_05c2ac4eba9b4cafbcc2994ed4be2a68",
              "IPY_MODEL_a1fbdf7768584e18bccaf729a19a1557"
            ],
            "layout": "IPY_MODEL_c46e0e4b3a3042ba9dacab5ef9a544fb"
          }
        },
        "a1c950a0a5ca4df4900c39d05db9b7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1f340ea700549e1930bcd875348db1b",
            "placeholder": "​",
            "style": "IPY_MODEL_a03851cff04843bbb4232071ab012fa3",
            "value": "vocab.json: 100%"
          }
        },
        "05c2ac4eba9b4cafbcc2994ed4be2a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a805f8063da54f9eae79503ffe5858ac",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16c732c5c9fe4a4ba15d589cf7ae9a80",
            "value": 1042301
          }
        },
        "a1fbdf7768584e18bccaf729a19a1557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3169bc38af64d7d82c7583a3b2de4b1",
            "placeholder": "​",
            "style": "IPY_MODEL_9e7a01414e0442f59d11a85568d64f39",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 4.22MB/s]"
          }
        },
        "c46e0e4b3a3042ba9dacab5ef9a544fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f340ea700549e1930bcd875348db1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a03851cff04843bbb4232071ab012fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a805f8063da54f9eae79503ffe5858ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c732c5c9fe4a4ba15d589cf7ae9a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3169bc38af64d7d82c7583a3b2de4b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e7a01414e0442f59d11a85568d64f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b551f140d2fd4d929e6806c67bfe9059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_814f87568e1642f2a90fb9189fc42e89",
              "IPY_MODEL_78531c4e2faa461da4926a70fae99a3e",
              "IPY_MODEL_632c361cac164656ae6ec0713868b43b"
            ],
            "layout": "IPY_MODEL_29d0c5adf34f4c8cbf6350b42a49ab81"
          }
        },
        "814f87568e1642f2a90fb9189fc42e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_730267ba7a7549e498edf5441a8e582f",
            "placeholder": "​",
            "style": "IPY_MODEL_5709510c085f41bfb8f93ad58d2e7d00",
            "value": "merges.txt: 100%"
          }
        },
        "78531c4e2faa461da4926a70fae99a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5d287f71a7044809ed14c5bd134d071",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92975249e22e4faead1f48c40bef3751",
            "value": 456318
          }
        },
        "632c361cac164656ae6ec0713868b43b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e49c959aa1324ca68152bd1d97c30af8",
            "placeholder": "​",
            "style": "IPY_MODEL_e3f2459ad8904c2aab4f95fafbc3b874",
            "value": " 456k/456k [00:00&lt;00:00, 2.81MB/s]"
          }
        },
        "29d0c5adf34f4c8cbf6350b42a49ab81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "730267ba7a7549e498edf5441a8e582f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5709510c085f41bfb8f93ad58d2e7d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5d287f71a7044809ed14c5bd134d071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92975249e22e4faead1f48c40bef3751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e49c959aa1324ca68152bd1d97c30af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f2459ad8904c2aab4f95fafbc3b874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37e2c535d8244c67891f004004386a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74acd5806fc24ec38b4595c1e26b7274",
              "IPY_MODEL_62fcb96daada4b79898e11ad34537b7f",
              "IPY_MODEL_905ddc4512a6433cba635a81d7dffe4e"
            ],
            "layout": "IPY_MODEL_559bcaf12ed84539b05e6569e231ef1f"
          }
        },
        "74acd5806fc24ec38b4595c1e26b7274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39004c2bb2044d399b55f5fa0fc34127",
            "placeholder": "​",
            "style": "IPY_MODEL_9e98cf6d91524356bdc3141ab134e4b9",
            "value": "tokenizer.json: 100%"
          }
        },
        "62fcb96daada4b79898e11ad34537b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d12c871e7fd4cfebafbe696ea90ebc9",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7aa6f05d75e4265a1c960cef3f09d33",
            "value": 1355256
          }
        },
        "905ddc4512a6433cba635a81d7dffe4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_825927bca19442e5a28835edd1fb91b3",
            "placeholder": "​",
            "style": "IPY_MODEL_f55b4a2486a045d3962e5270a7d1a743",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 5.43MB/s]"
          }
        },
        "559bcaf12ed84539b05e6569e231ef1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39004c2bb2044d399b55f5fa0fc34127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e98cf6d91524356bdc3141ab134e4b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d12c871e7fd4cfebafbe696ea90ebc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7aa6f05d75e4265a1c960cef3f09d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "825927bca19442e5a28835edd1fb91b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f55b4a2486a045d3962e5270a7d1a743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d9bf7a2a2544a2bbe05677cf656cf88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a4163607a5740a4baedb07c1b84a3c1",
              "IPY_MODEL_8ad5ce93a8e044f2bbc29a466a43aa1b",
              "IPY_MODEL_606c70a0648547009dfcbc506bfc6786"
            ],
            "layout": "IPY_MODEL_2cd8b0b0ae65427da0459919e71586c1"
          }
        },
        "5a4163607a5740a4baedb07c1b84a3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9f4896ab6fb496e97976c78672f2d92",
            "placeholder": "​",
            "style": "IPY_MODEL_7aafe63a8e9d4c5b937e4b18bd60a407",
            "value": "config.json: 100%"
          }
        },
        "8ad5ce93a8e044f2bbc29a466a43aa1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dfadcda386e464eb7dff1e4f0bb5807",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83f6bd116d7e4a1da8d9757022a0042a",
            "value": 665
          }
        },
        "606c70a0648547009dfcbc506bfc6786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb956bbae8c64cde89e83376cbc8f91e",
            "placeholder": "​",
            "style": "IPY_MODEL_6564cb0d456245e7b5381f8043104ca3",
            "value": " 665/665 [00:00&lt;00:00, 10.4kB/s]"
          }
        },
        "2cd8b0b0ae65427da0459919e71586c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9f4896ab6fb496e97976c78672f2d92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aafe63a8e9d4c5b937e4b18bd60a407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dfadcda386e464eb7dff1e4f0bb5807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f6bd116d7e4a1da8d9757022a0042a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb956bbae8c64cde89e83376cbc8f91e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6564cb0d456245e7b5381f8043104ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfc7fd4fcc6848988e1436ae8fde016b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70df94d7f7244a3cb4cd0dacc979b046",
              "IPY_MODEL_82d211cb769845229d66978f73280b60",
              "IPY_MODEL_628cf1279a554f38aa86cd784f306ce6"
            ],
            "layout": "IPY_MODEL_ab95f0f4934b44e58aa807822df04575"
          }
        },
        "70df94d7f7244a3cb4cd0dacc979b046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76a93d6fed0847b49fdfc744e2b8b902",
            "placeholder": "​",
            "style": "IPY_MODEL_ba546a5f2d6e49ceb7d21f60810ecc81",
            "value": "model.safetensors: 100%"
          }
        },
        "82d211cb769845229d66978f73280b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e50f2ff03d544f3a5e53b407bed407e",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83733b61ea0e4501874a7b08758372ac",
            "value": 548105171
          }
        },
        "628cf1279a554f38aa86cd784f306ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78786bbcb3ce41e4b51a7ceb04ef76cf",
            "placeholder": "​",
            "style": "IPY_MODEL_9e9d8ebb2b95422f881a433179c89862",
            "value": " 548M/548M [00:12&lt;00:00, 61.4MB/s]"
          }
        },
        "ab95f0f4934b44e58aa807822df04575": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76a93d6fed0847b49fdfc744e2b8b902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba546a5f2d6e49ceb7d21f60810ecc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e50f2ff03d544f3a5e53b407bed407e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83733b61ea0e4501874a7b08758372ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78786bbcb3ce41e4b51a7ceb04ef76cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e9d8ebb2b95422f881a433179c89862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "550cce0084ac4f938f9d62aaca61a685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91be60ef98c249e2a551d5930b8fd699",
              "IPY_MODEL_02920ebdd36b409ab9473b60b9e24fcd",
              "IPY_MODEL_005b26f0e9b04902bcbb1db08f21d575"
            ],
            "layout": "IPY_MODEL_3ca2c613130a4a5e83546e3cabf482b6"
          }
        },
        "91be60ef98c249e2a551d5930b8fd699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed9d19fb6078490282e8f49f394ee93e",
            "placeholder": "​",
            "style": "IPY_MODEL_ab03e5b03c9f4ad6b4198e79aaf8d8ad",
            "value": "generation_config.json: 100%"
          }
        },
        "02920ebdd36b409ab9473b60b9e24fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48a618e6ef24961981028de6a38ee3d",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c3e495eb9ac41f095eec3cad0ad8aa0",
            "value": 124
          }
        },
        "005b26f0e9b04902bcbb1db08f21d575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d2098d42024a639edc11be7b3ec3ba",
            "placeholder": "​",
            "style": "IPY_MODEL_b3daf9782f2e422c9eeaf5b81fd5db36",
            "value": " 124/124 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "3ca2c613130a4a5e83546e3cabf482b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9d19fb6078490282e8f49f394ee93e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab03e5b03c9f4ad6b4198e79aaf8d8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e48a618e6ef24961981028de6a38ee3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c3e495eb9ac41f095eec3cad0ad8aa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41d2098d42024a639edc11be7b3ec3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3daf9782f2e422c9eeaf5b81fd5db36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92ac881eec804725a1894537e72b94f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bbeb270b9864113a3e0043e5ba9ece4",
              "IPY_MODEL_3afbe342707d4e44b29adf86070e4ca7",
              "IPY_MODEL_75a07e0b13a84dde89e367667d8788ab"
            ],
            "layout": "IPY_MODEL_818036356c1d4ef58faed6416d6e0809"
          }
        },
        "2bbeb270b9864113a3e0043e5ba9ece4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2346262864fd4aa494e75c81e9cd2a6f",
            "placeholder": "​",
            "style": "IPY_MODEL_9f8a357e272e46998f4ef9819f5ecd49",
            "value": "Generating train split: "
          }
        },
        "3afbe342707d4e44b29adf86070e4ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b4d139e61b24bdfbfd8a98cab708895",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2bf6749523943d58b60fa0dc587d8f3",
            "value": 1
          }
        },
        "75a07e0b13a84dde89e367667d8788ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1982466c67a49038b8167663b72563e",
            "placeholder": "​",
            "style": "IPY_MODEL_3972bd7e5072462eb03860742a91bec1",
            "value": " 424/0 [00:00&lt;00:00, 21668.07 examples/s]"
          }
        },
        "818036356c1d4ef58faed6416d6e0809": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2346262864fd4aa494e75c81e9cd2a6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f8a357e272e46998f4ef9819f5ecd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b4d139e61b24bdfbfd8a98cab708895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a2bf6749523943d58b60fa0dc587d8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1982466c67a49038b8167663b72563e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3972bd7e5072462eb03860742a91bec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3030d40eac94a38bab2c73715eaf65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2a3ad4d4b204bb1b74e3676d2df0204",
              "IPY_MODEL_ec1b79cb58e64eb7ac2bb361e2ec59cc",
              "IPY_MODEL_6f8647d147ef4732ac13e0068b6e9133"
            ],
            "layout": "IPY_MODEL_6a6d814b4ef34eb7bfed4ddb6e4989c0"
          }
        },
        "b2a3ad4d4b204bb1b74e3676d2df0204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17826c3b3cb14642aa16be9c3a60d536",
            "placeholder": "​",
            "style": "IPY_MODEL_4cbb2c1b3c1d487fae55b249b82e146b",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "ec1b79cb58e64eb7ac2bb361e2ec59cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f7f068a2be149f284771b7233f8e62b",
            "max": 424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a4c0cbc31854bfa97a877abff8eb522",
            "value": 424
          }
        },
        "6f8647d147ef4732ac13e0068b6e9133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15c99e49fde84fa08648619b2dbdc00c",
            "placeholder": "​",
            "style": "IPY_MODEL_34a910fb5d3747968196eb7a78bce399",
            "value": " 424/424 [00:00&lt;00:00, 885.86 examples/s]"
          }
        },
        "6a6d814b4ef34eb7bfed4ddb6e4989c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17826c3b3cb14642aa16be9c3a60d536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cbb2c1b3c1d487fae55b249b82e146b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f7f068a2be149f284771b7233f8e62b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4c0cbc31854bfa97a877abff8eb522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15c99e49fde84fa08648619b2dbdc00c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a910fb5d3747968196eb7a78bce399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patricknelwan/ye-trump-GPT/blob/main/ye_trump_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQx-7PCyeMBv",
        "outputId": "8f7305da-ba99-4c81-e7e3-065d33b3899b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, transformers, datasets, bitsandbytes\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.1\n",
            "    Uninstalling transformers-4.56.1:\n",
            "      Successfully uninstalled transformers-4.56.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.47.0 datasets-4.1.1 pyarrow-21.0.0 transformers-4.56.2\n",
            "Collecting torch-pruning\n",
            "  Downloading torch_pruning-1.6.1-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from torch-pruning) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-pruning) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->torch-pruning) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->torch-pruning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->torch-pruning) (3.0.2)\n",
            "Downloading torch_pruning-1.6.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.2/70.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-pruning\n",
            "Successfully installed torch-pruning-1.6.1\n",
            "Collecting git+https://github.com/huggingface/peft\n",
            "  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-fq7wjk9l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-fq7wjk9l\n",
            "  Resolved https://github.com/huggingface/peft to commit 6030f9160ed2fc17220f6f41382a66f1257b6a93\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (1.10.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.2.dev0) (0.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.2.dev0) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.2.dev0) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.17.2.dev0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.17.2.dev0) (0.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.17.2.dev0) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.2.dev0) (2025.8.3)\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.17.2.dev0-py3-none-any.whl size=505094 sha256=c2bbd795d7d3e3cf28f308badafb3598321950a55779694863185e7155dc3bde\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qxetb2mu/wheels/92/c9/f9/6d1893af725a9b89369aa9416c272f2d512491873dd9db54cd\n",
            "Successfully built peft\n",
            "Installing collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.1\n",
            "    Uninstalling peft-0.17.1:\n",
            "      Successfully uninstalled peft-0.17.1\n",
            "Successfully installed peft-0.17.2.dev0\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,628 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,363 kB]\n",
            "Get:12 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,584 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,577 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,804 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,690 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,307 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,808 kB]\n",
            "Fetched 33.8 MB in 3s (10.4 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers datasets torch accelerate peft bitsandbytes sentencepiece\n",
        "!pip install torch-pruning\n",
        "!pip install git+https://github.com/huggingface/peft\n",
        "!pip install einops\n",
        "\n",
        "!apt-get update && apt-get install build-essential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = ['kanye-quotes.txt', 'trump-quotes.txt']\n",
        "\n",
        "with open('merged.txt', 'w', encoding='utf-8') as outfile:\n",
        "    for fname in files:\n",
        "        try:\n",
        "            with open(fname, 'r', encoding='utf-8') as infile:\n",
        "                content = infile.read()\n",
        "                outfile.write(content + \"\\n\\n\")\n",
        "                print(f\"✅ Added {fname} to merged dataset\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"[!] {fname} not found - skipping\")\n",
        "\n",
        "    print(\"🎤🇺🇸 Kanye + Trump dataset merged successfully!\")\n",
        "\n",
        "with open('merged.txt', 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "    print(f\"📊 Total characters: {len(content):,}\")\n",
        "    print(f\"📄 Total lines: {len(content.splitlines()):,}\")\n",
        "    print(f\"\\n📝 Preview (first 200 characters):\")\n",
        "    print(content[:200] + \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRFNHBCS3zDt",
        "outputId": "96c24896-6bbe-4f00-d465-22117275b28a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Added kanye-quotes.txt to merged dataset\n",
            "✅ Added trump-quotes.txt to merged dataset\n",
            "🎤🇺🇸 Kanye + Trump dataset merged successfully!\n",
            "📊 Total characters: 14,182\n",
            "📄 Total lines: 424\n",
            "\n",
            "📝 Preview (first 200 characters):\n",
            "\"People always say that you can't please everybody. I think that's a cop-out. Why not attempt it? Cause think of all the people that you will please if you try.\"\n",
            "\n",
            "\"If I got any cooler I would freeze t...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer , GPT2LMHeadModel , Trainer , TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token =tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "dataset = load_dataset('text' , data_files = 'merged.txt')\n",
        "def tokenize_function(examples) :\n",
        "  return tokenizer(examples[\"text\"] , truncation = True , padding = True , max_length = 128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432,
          "referenced_widgets": [
            "559608c2edf74a01a89edcaa5ddc8e66",
            "cc60e8ff4e73432ea33f571f5ed00033",
            "fd86a3f633a742b4aedf814f92f040f3",
            "a7d0084efca64c8f8d5d6231a242277b",
            "de3da74f1414449791c2ea6f6a243b3c",
            "f8d28c9106e04a2caa551f0eb84299b1",
            "782c3598ff4c4b3ba92c7c740693c714",
            "e00652fd8a784f2796256085e46fd518",
            "4c4893fabd974809a197b422d77900d9",
            "6df83872a57a4d83b14a4fd9b261f599",
            "7be424b000e94e459a9e79dfb36c120c",
            "7526cbbf6e2947e7ad4726cb4d9c6dac",
            "a1c950a0a5ca4df4900c39d05db9b7dc",
            "05c2ac4eba9b4cafbcc2994ed4be2a68",
            "a1fbdf7768584e18bccaf729a19a1557",
            "c46e0e4b3a3042ba9dacab5ef9a544fb",
            "f1f340ea700549e1930bcd875348db1b",
            "a03851cff04843bbb4232071ab012fa3",
            "a805f8063da54f9eae79503ffe5858ac",
            "16c732c5c9fe4a4ba15d589cf7ae9a80",
            "a3169bc38af64d7d82c7583a3b2de4b1",
            "9e7a01414e0442f59d11a85568d64f39",
            "b551f140d2fd4d929e6806c67bfe9059",
            "814f87568e1642f2a90fb9189fc42e89",
            "78531c4e2faa461da4926a70fae99a3e",
            "632c361cac164656ae6ec0713868b43b",
            "29d0c5adf34f4c8cbf6350b42a49ab81",
            "730267ba7a7549e498edf5441a8e582f",
            "5709510c085f41bfb8f93ad58d2e7d00",
            "e5d287f71a7044809ed14c5bd134d071",
            "92975249e22e4faead1f48c40bef3751",
            "e49c959aa1324ca68152bd1d97c30af8",
            "e3f2459ad8904c2aab4f95fafbc3b874",
            "37e2c535d8244c67891f004004386a36",
            "74acd5806fc24ec38b4595c1e26b7274",
            "62fcb96daada4b79898e11ad34537b7f",
            "905ddc4512a6433cba635a81d7dffe4e",
            "559bcaf12ed84539b05e6569e231ef1f",
            "39004c2bb2044d399b55f5fa0fc34127",
            "9e98cf6d91524356bdc3141ab134e4b9",
            "9d12c871e7fd4cfebafbe696ea90ebc9",
            "f7aa6f05d75e4265a1c960cef3f09d33",
            "825927bca19442e5a28835edd1fb91b3",
            "f55b4a2486a045d3962e5270a7d1a743",
            "2d9bf7a2a2544a2bbe05677cf656cf88",
            "5a4163607a5740a4baedb07c1b84a3c1",
            "8ad5ce93a8e044f2bbc29a466a43aa1b",
            "606c70a0648547009dfcbc506bfc6786",
            "2cd8b0b0ae65427da0459919e71586c1",
            "c9f4896ab6fb496e97976c78672f2d92",
            "7aafe63a8e9d4c5b937e4b18bd60a407",
            "8dfadcda386e464eb7dff1e4f0bb5807",
            "83f6bd116d7e4a1da8d9757022a0042a",
            "bb956bbae8c64cde89e83376cbc8f91e",
            "6564cb0d456245e7b5381f8043104ca3",
            "bfc7fd4fcc6848988e1436ae8fde016b",
            "70df94d7f7244a3cb4cd0dacc979b046",
            "82d211cb769845229d66978f73280b60",
            "628cf1279a554f38aa86cd784f306ce6",
            "ab95f0f4934b44e58aa807822df04575",
            "76a93d6fed0847b49fdfc744e2b8b902",
            "ba546a5f2d6e49ceb7d21f60810ecc81",
            "9e50f2ff03d544f3a5e53b407bed407e",
            "83733b61ea0e4501874a7b08758372ac",
            "78786bbcb3ce41e4b51a7ceb04ef76cf",
            "9e9d8ebb2b95422f881a433179c89862",
            "550cce0084ac4f938f9d62aaca61a685",
            "91be60ef98c249e2a551d5930b8fd699",
            "02920ebdd36b409ab9473b60b9e24fcd",
            "005b26f0e9b04902bcbb1db08f21d575",
            "3ca2c613130a4a5e83546e3cabf482b6",
            "ed9d19fb6078490282e8f49f394ee93e",
            "ab03e5b03c9f4ad6b4198e79aaf8d8ad",
            "e48a618e6ef24961981028de6a38ee3d",
            "6c3e495eb9ac41f095eec3cad0ad8aa0",
            "41d2098d42024a639edc11be7b3ec3ba",
            "b3daf9782f2e422c9eeaf5b81fd5db36",
            "92ac881eec804725a1894537e72b94f5",
            "2bbeb270b9864113a3e0043e5ba9ece4",
            "3afbe342707d4e44b29adf86070e4ca7",
            "75a07e0b13a84dde89e367667d8788ab",
            "818036356c1d4ef58faed6416d6e0809",
            "2346262864fd4aa494e75c81e9cd2a6f",
            "9f8a357e272e46998f4ef9819f5ecd49",
            "1b4d139e61b24bdfbfd8a98cab708895",
            "a2bf6749523943d58b60fa0dc587d8f3",
            "a1982466c67a49038b8167663b72563e",
            "3972bd7e5072462eb03860742a91bec1",
            "e3030d40eac94a38bab2c73715eaf65a",
            "b2a3ad4d4b204bb1b74e3676d2df0204",
            "ec1b79cb58e64eb7ac2bb361e2ec59cc",
            "6f8647d147ef4732ac13e0068b6e9133",
            "6a6d814b4ef34eb7bfed4ddb6e4989c0",
            "17826c3b3cb14642aa16be9c3a60d536",
            "4cbb2c1b3c1d487fae55b249b82e146b",
            "8f7f068a2be149f284771b7233f8e62b",
            "4a4c0cbc31854bfa97a877abff8eb522",
            "15c99e49fde84fa08648619b2dbdc00c",
            "34a910fb5d3747968196eb7a78bce399"
          ]
        },
        "id": "rXasHFsE4dX-",
        "outputId": "24c63eb2-cfda-4f26-91be-5e8ad0ede6da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "559608c2edf74a01a89edcaa5ddc8e66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7526cbbf6e2947e7ad4726cb4d9c6dac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b551f140d2fd4d929e6806c67bfe9059"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37e2c535d8244c67891f004004386a36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d9bf7a2a2544a2bbe05677cf656cf88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfc7fd4fcc6848988e1436ae8fde016b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "550cce0084ac4f938f9d62aaca61a685"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92ac881eec804725a1894537e72b94f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/424 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3030d40eac94a38bab2c73715eaf65a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ppG6_Gi-GqG",
        "outputId": "1e2c997c-b46c-4373-82e5-1946c3a97273"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-ye-trump\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    save_steps=100,\n",
        "    logging_steps=50,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_dir=\"logs\",\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1,\n",
        "\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"training..\")\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"gpt2-ye-trump\")\n",
        "tokenizer.save_pretrained(\"gpt2-ye-trump\")\n",
        "\n",
        "print(\"saved to 'gpt2-ye-trump'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "collapsed": true,
        "id": "teYq39CY5i5N",
        "outputId": "a0d8ee6b-edfe-4e63-9e51-3e6f429b766e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training..\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [54/54 00:38, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.429000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved to 'gpt2-ye-trump'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    \"gpt2-ye-trump\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    dtype=torch.float16,\n",
        ")\n"
      ],
      "metadata": {
        "id": "KHxhHkNg5kEb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-ye-trump\")\n",
        "model.eval()\n",
        "weighsToSparce = 0.5\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        with torch.no_grad():\n",
        "            w = module.weight.data\n",
        "            mask = torch.rand(w.shape) > weighsToSparce\n",
        "            w *= mask.to(w.device)\n",
        "\n",
        "print(\"Model weights sparsified (\" , weighsToSparce , \") zeroed out)\")\n",
        "\n",
        "model.save_pretrained(\"gpt2-ye-trump-sparse\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_9hsfxjCLJO",
        "outputId": "a2fbe5a2-8855-433b-bb12-440e9f699688"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights sparsified ( 0.5 ) zeroed out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral-common\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "259lq8S1WN0m",
        "outputId": "2a05d037-15ec-4412-c7a8-2863214d13d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistral-common\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.11.9)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.15.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (0.11.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.0.2)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.4.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral-common) (2024.11.6)\n",
            "Downloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry, pydantic-extra-types, mistral-common\n",
            "Successfully installed mistral-common-1.8.5 pycountry-24.6.1 pydantic-extra-types-2.10.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-ye-trump\")\n",
        "tokenizer.save_pretrained(\"gpt2-ye-trump\")\n",
        "\n",
        "!rm -rf llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!pip install sentencepiece tokenizers gguf mistral-common\n",
        "\n",
        "!cd llama.cpp && mkdir -p build && cd build && \\\n",
        "cmake .. \\\n",
        "-DCMAKE_BUILD_TYPE=Release \\\n",
        "&& make -j\n",
        "\n",
        "!cp -r gpt2-ye-trump llama.cpp/models/gpt2-ye-trump\n",
        "\n",
        "!cd llama.cpp && python3 convert_hf_to_gguf.py models/gpt2-ye-trump --outfile models/gpt2-ye-trump.gguf\n",
        "\n",
        "!cd llama.cpp && ./build/bin/llama-quantize models/gpt2-ye-trump.gguf models/gpt2-ye-trump-Q2_K.gguf Q2_K\n",
        "\n",
        "!ls -lh llama.cpp/models/*.gguf\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT0GhxbIG2hk",
        "outputId": "9d97485a-37a3-41f7-9867-18f9e4de388e",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 63036, done.\u001b[K\n",
            "remote: Counting objects: 100% (159/159), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 63036 (delta 98), reused 42 (delta 39), pack-reused 62877 (from 4)\u001b[K\n",
            "Receiving objects: 100% (63036/63036), 157.41 MiB | 13.78 MiB/s, done.\n",
            "Resolving deltas: 100% (45727/45727), done.\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.0)\n",
            "Collecting gguf\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: mistral-common in /usr/local/lib/python3.12/dist-packages (1.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from gguf) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from gguf) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from gguf) (4.67.1)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.11.9)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (4.15.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (0.11.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common) (2.32.4)\n",
            "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common) (2.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.10)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.4.1)\n",
            "Requirement already satisfied: pycountry>=23 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common) (24.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral-common) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral-common) (2024.11.6)\n",
            "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gguf\n",
            "Successfully installed gguf-0.17.1\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.9.0-dev\n",
            "-- ggml commit:  b05a9d65\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.9s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  7%] Built target build_info\n",
            "[  7%] Built target sha1\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 10%] Built target sha256\n",
            "[ 10%] Built target llama-llava-cli\n",
            "[ 10%] Built target llama-gemma3-cli\n",
            "[ 10%] Built target llama-qwen2vl-cli\n",
            "[ 10%] Built target llama-minicpmv-cli\n",
            "[ 10%] Built target xxhash\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[ 10%] Built target ggml-base\n",
            "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 16%] Built target ggml-cpu\n",
            "[ 16%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 16%] Built target ggml\n",
            "[ 17%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 28%] Built target llama-gguf\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 28%] Built target llama-gguf-hash\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 28%] Built target llama\n",
            "[ 28%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 35%] Built target test-c\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 35%] Built target llama-simple\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 36%] Built target llama-simple-chat\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 37%] Built target mtmd\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 38%] Built target common\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 73%] Built target test-mtmd-c-api\n",
            "[ 73%] Built target test-model-load-cancel\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 73%] Built target test-log\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 73%] Built target test-rope\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 74%] Built target test-autorelease\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 74%] Built target test-barrier\n",
            "[ 74%] Built target test-quantize-fns\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 76%] Built target llama-q8dot\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 77%] Built target llama-logits\n",
            "[ 77%] Built target llama-lookup-merge\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 77%] Built target llama-vdot\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 77%] Built target test-gbnf-validator\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
            "[ 78%] Built target test-tokenizer-1-spm\n",
            "[ 78%] Built target test-tokenizer-1-bpe\n",
            "[ 78%] Built target llama-tokenize\n",
            "[ 78%] Built target test-alloc\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 79%] Built target test-sampling\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 79%] Built target llama-gguf-split\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 81%] Built target test-grammar-parser\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 84%] Built target test-regex-partial\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 85%] Built target test-tokenizer-0\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 86%] Built target test-llama-grammar\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 87%] Built target llama-finetune\n",
            "[ 87%] Built target llama-lookup-create\n",
            "[ 87%] Built target test-quantize-perf\n",
            "[ 87%] Built target llama-gen-docs\n",
            "[ 87%] Built target llama-eval-callback\n",
            "[ 87%] Built target llama-save-load-state\n",
            "[ 87%] Built target llama-batched\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 88%] Built target test-thread-safety\n",
            "[ 88%] Built target llama-speculative-simple\n",
            "[ 88%] Built target llama-batched-bench\n",
            "[ 88%] Built target llama-lookup-stats\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 88%] Built target llama-passkey\n",
            "[ 88%] Built target llama-lookup\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 88%] Built target test-opt\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 89%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 90%] Built target test-arg-parser\n",
            "[ 90%] Built target test-gguf\n",
            "[ 90%] Built target llama-lookahead\n",
            "[ 90%] Built target llama-embedding\n",
            "[ 90%] Built target llama-parallel\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 91%] Built target llama-quantize\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 91%] Built target llama-retrieval\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 92%] Built target llama-mtmd-cli\n",
            "[ 92%] Built target llama-export-lora\n",
            "[ 92%] Built target test-chat-template\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 92%] Built target llama-diffusion-cli\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 94%] Built target llama-cvector-generator\n",
            "[ 94%] Built target test-json-partial\n",
            "[ 94%] Built target llama-speculative\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 94%] Built target llama-cli\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 94%] Built target test-quantize-stats\n",
            "[ 94%] Built target llama-perplexity\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 95%] Built target test-grammar-integration\n",
            "[ 95%] Built target llama-run\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 96%] Built target test-chat-parser\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 98%] Built target llama-imatrix\n",
            "[ 98%] Built target test-json-schema-to-grammar\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 99%] Built target llama-bench\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 99%] Built target llama-tts\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[100%] Built target test-chat\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[100%] Built target test-backend-ops\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n",
            "INFO:hf-to-gguf:Loading model: gpt2-ye-trump\n",
            "INFO:hf-to-gguf:Model architecture: GPT2LMHeadModel\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "INFO:hf-to-gguf:blk.0.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.1.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.10.attn_qkv.bias,      torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.bias,   torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.bias,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.bias,        torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.11.attn_qkv.bias,      torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.bias,   torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.bias,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.bias,        torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.2.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.3.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.4.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.5.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.6.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.7.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.8.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:blk.9.attn_qkv.bias,       torch.float32 --> F32, shape = {2304}\n",
            "INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float32 --> F16, shape = {768, 2304}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.bias,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {768, 768}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.bias,      torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.bias,         torch.float32 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {768, 3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.bias,       torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {3072, 768}\n",
            "INFO:hf-to-gguf:output_norm.bias,          torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {768}\n",
            "INFO:hf-to-gguf:position_embd.weight,      torch.float32 --> F32, shape = {768, 1024}\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.float32 --> F16, shape = {768, 50257}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 50000 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 50256\n",
            "INFO:gguf.vocab:Setting special token type eos to 50256\n",
            "INFO:gguf.vocab:Setting special token type unk to 50256\n",
            "INFO:gguf.vocab:Setting special token type pad to 50256\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:models/gpt2-ye-trump.gguf: n_tensors = 148, total_size = 250.7M\n",
            "Writing: 100% 251M/251M [00:04<00:00, 57.6Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to models/gpt2-ye-trump.gguf\n",
            "main: build = 6585 (b05a9d65)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'models/gpt2-ye-trump.gguf' to 'models/gpt2-ye-trump-Q2_K.gguf' as Q2_K\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 148 tensors from models/gpt2-ye-trump.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gpt2 Ye Trump\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 124M\n",
            "llama_model_loader: - kv   4:                           gpt2.block_count u32              = 12\n",
            "llama_model_loader: - kv   5:                        gpt2.context_length u32              = 1024\n",
            "llama_model_loader: - kv   6:                      gpt2.embedding_length u32              = 768\n",
            "llama_model_loader: - kv   7:                   gpt2.feed_forward_length u32              = 3072\n",
            "llama_model_loader: - kv   8:                  gpt2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv   9:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = gpt-2\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,50257]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,50257]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 50256\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - type  f32:   99 tensors\n",
            "llama_model_loader: - type  f16:   49 tensors\n",
            "[   1/ 148]                     output_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[   2/ 148]                   output_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[   3/ 148]                 position_embd.weight - [  768,  1024,     1,     1], type =    f32, size =    3.000 MiB\n",
            "[   4/ 148]                    token_embd.weight - [  768, 50257,     1,     1], type =    f16, converting to q6_K .. size =    73.62 MiB ->    30.20 MiB\n",
            "[   5/ 148]                 blk.0.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[   6/ 148]               blk.0.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[   7/ 148]               blk.0.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[   8/ 148]             blk.0.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[   9/ 148]                  blk.0.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[  10/ 148]                blk.0.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  11/ 148]                  blk.0.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  12/ 148]                blk.0.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  13/ 148]                  blk.0.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  14/ 148]                blk.0.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  15/ 148]                    blk.0.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  16/ 148]                  blk.0.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  17/ 148]                 blk.1.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  18/ 148]               blk.1.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  19/ 148]               blk.1.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  20/ 148]             blk.1.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  21/ 148]                  blk.1.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[  22/ 148]                blk.1.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  23/ 148]                  blk.1.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  24/ 148]                blk.1.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  25/ 148]                  blk.1.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  26/ 148]                blk.1.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  27/ 148]                    blk.1.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  28/ 148]                  blk.1.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  29/ 148]                 blk.2.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  30/ 148]               blk.2.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  31/ 148]               blk.2.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  32/ 148]             blk.2.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  33/ 148]                  blk.2.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[  34/ 148]                blk.2.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  35/ 148]                  blk.2.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  36/ 148]                blk.2.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  37/ 148]                  blk.2.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  38/ 148]                blk.2.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  39/ 148]                    blk.2.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  40/ 148]                  blk.2.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  41/ 148]                 blk.3.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  42/ 148]               blk.3.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  43/ 148]               blk.3.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  44/ 148]             blk.3.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  45/ 148]                  blk.3.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[  46/ 148]                blk.3.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  47/ 148]                  blk.3.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  48/ 148]                blk.3.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  49/ 148]                  blk.3.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  50/ 148]                blk.3.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  51/ 148]                    blk.3.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  52/ 148]                  blk.3.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  53/ 148]                 blk.4.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  54/ 148]               blk.4.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  55/ 148]               blk.4.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  56/ 148]             blk.4.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  57/ 148]                  blk.4.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[  58/ 148]                blk.4.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  59/ 148]                  blk.4.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  60/ 148]                blk.4.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  61/ 148]                  blk.4.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  62/ 148]                blk.4.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  63/ 148]                    blk.4.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  64/ 148]                  blk.4.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  65/ 148]                 blk.5.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  66/ 148]               blk.5.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  67/ 148]               blk.5.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  68/ 148]             blk.5.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  69/ 148]                  blk.5.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[  70/ 148]                blk.5.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  71/ 148]                  blk.5.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  72/ 148]                blk.5.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  73/ 148]                  blk.5.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  74/ 148]                blk.5.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  75/ 148]                    blk.5.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  76/ 148]                  blk.5.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  77/ 148]                 blk.6.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  78/ 148]               blk.6.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  79/ 148]               blk.6.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  80/ 148]             blk.6.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  81/ 148]                  blk.6.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[  82/ 148]                blk.6.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  83/ 148]                  blk.6.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  84/ 148]                blk.6.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  85/ 148]                  blk.6.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  86/ 148]                blk.6.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  87/ 148]                    blk.6.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[  88/ 148]                  blk.6.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[  89/ 148]                 blk.7.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  90/ 148]               blk.7.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  91/ 148]               blk.7.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  92/ 148]             blk.7.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[  93/ 148]                  blk.7.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[  94/ 148]                blk.7.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[  95/ 148]                  blk.7.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  96/ 148]                blk.7.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[  97/ 148]                  blk.7.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  98/ 148]                blk.7.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[  99/ 148]                    blk.7.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 100/ 148]                  blk.7.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[ 101/ 148]                 blk.8.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 102/ 148]               blk.8.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 103/ 148]               blk.8.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 104/ 148]             blk.8.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[ 105/ 148]                  blk.8.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[ 106/ 148]                blk.8.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[ 107/ 148]                  blk.8.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 108/ 148]                blk.8.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[ 109/ 148]                  blk.8.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 110/ 148]                blk.8.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 111/ 148]                    blk.8.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 112/ 148]                  blk.8.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[ 113/ 148]                 blk.9.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 114/ 148]               blk.9.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 115/ 148]               blk.9.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 116/ 148]             blk.9.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[ 117/ 148]                  blk.9.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[ 118/ 148]                blk.9.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[ 119/ 148]                  blk.9.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 120/ 148]                blk.9.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[ 121/ 148]                  blk.9.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 122/ 148]                blk.9.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 123/ 148]                    blk.9.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 124/ 148]                  blk.9.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[ 125/ 148]                blk.10.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 126/ 148]              blk.10.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 127/ 148]              blk.10.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 128/ 148]            blk.10.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[ 129/ 148]                 blk.10.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[ 130/ 148]               blk.10.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[ 131/ 148]                 blk.10.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 132/ 148]               blk.10.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[ 133/ 148]                 blk.10.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 134/ 148]               blk.10.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 135/ 148]                   blk.10.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 136/ 148]                 blk.10.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "[ 137/ 148]                blk.11.attn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 138/ 148]              blk.11.attn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 139/ 148]              blk.11.attn_output.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 140/ 148]            blk.11.attn_output.weight - [  768,   768,     1,     1], type =    f16, converting to q3_K .. size =     1.12 MiB ->     0.24 MiB\n",
            "[ 141/ 148]                 blk.11.attn_qkv.bias - [ 2304,     1,     1,     1], type =    f32, size =    0.009 MiB\n",
            "[ 142/ 148]               blk.11.attn_qkv.weight - [  768,  2304,     1,     1], type =    f16, converting to q2_K .. size =     3.38 MiB ->     0.55 MiB\n",
            "[ 143/ 148]                 blk.11.ffn_down.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 144/ 148]               blk.11.ffn_down.weight - [ 3072,   768,     1,     1], type =    f16, converting to q3_K .. size =     4.50 MiB ->     0.97 MiB\n",
            "[ 145/ 148]                 blk.11.ffn_norm.bias - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 146/ 148]               blk.11.ffn_norm.weight - [  768,     1,     1,     1], type =    f32, size =    0.003 MiB\n",
            "[ 147/ 148]                   blk.11.ffn_up.bias - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MiB\n",
            "[ 148/ 148]                 blk.11.ffn_up.weight - [  768,  3072,     1,     1], type =    f16, converting to q2_K .. size =     4.50 MiB ->     0.74 MiB\n",
            "llama_model_quantize_impl: model size  =   239.08 MiB\n",
            "llama_model_quantize_impl: quant size  =    63.66 MiB\n",
            "\n",
            "main: quantize time =  8343.76 ms\n",
            "main:    total time =  8343.76 ms\n",
            "-rw-r--r-- 1 root root 4.7M Sep 25 16:24 llama.cpp/models/ggml-vocab-aquila.gguf\n",
            "-rw-r--r-- 1 root root 1.3M Sep 25 16:24 llama.cpp/models/ggml-vocab-baichuan.gguf\n",
            "-rw-r--r-- 1 root root 613K Sep 25 16:24 llama.cpp/models/ggml-vocab-bert-bge.gguf\n",
            "-rw-r--r-- 1 root root  11M Sep 25 16:24 llama.cpp/models/ggml-vocab-command-r.gguf\n",
            "-rw-r--r-- 1 root root 1.2M Sep 25 16:24 llama.cpp/models/ggml-vocab-deepseek-coder.gguf\n",
            "-rw-r--r-- 1 root root 3.8M Sep 25 16:24 llama.cpp/models/ggml-vocab-deepseek-llm.gguf\n",
            "-rw-r--r-- 1 root root 2.2M Sep 25 16:24 llama.cpp/models/ggml-vocab-falcon.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep 25 16:24 llama.cpp/models/ggml-vocab-gpt-2.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep 25 16:24 llama.cpp/models/ggml-vocab-gpt-neox.gguf\n",
            "-rw-r--r-- 1 root root 7.5M Sep 25 16:24 llama.cpp/models/ggml-vocab-llama-bpe.gguf\n",
            "-rw-r--r-- 1 root root 707K Sep 25 16:24 llama.cpp/models/ggml-vocab-llama-spm.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep 25 16:24 llama.cpp/models/ggml-vocab-mpt.gguf\n",
            "-rw-r--r-- 1 root root 6.6M Sep 25 16:24 llama.cpp/models/ggml-vocab-nomic-bert-moe.gguf\n",
            "-rw-r--r-- 1 root root 710K Sep 25 16:24 llama.cpp/models/ggml-vocab-phi-3.gguf\n",
            "-rw-r--r-- 1 root root 5.7M Sep 25 16:24 llama.cpp/models/ggml-vocab-qwen2.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep 25 16:24 llama.cpp/models/ggml-vocab-refact.gguf\n",
            "-rw-r--r-- 1 root root 1.7M Sep 25 16:24 llama.cpp/models/ggml-vocab-starcoder.gguf\n",
            "-rw-r--r-- 1 root root 241M Sep 25 16:34 llama.cpp/models/gpt2-ye-trump.gguf\n",
            "-rw-r--r-- 1 root root  66M Sep 25 16:34 llama.cpp/models/gpt2-ye-trump-Q2_K.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && ./build/bin/llama-cli -m models/gpt2-ye-trump-Q2_K.gguf -p \"hole \" -n 100 --temp 1 --no-warmup --repeat_penalty 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4vBS8yQWYLd",
        "outputId": "2aad7a76-c023-462b-9765-3dd5f07139a8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 6585 (b05a9d65) with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 148 tensors from models/gpt2-ye-trump-Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gpt2 Ye Trump\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 124M\n",
            "llama_model_loader: - kv   4:                           gpt2.block_count u32              = 12\n",
            "llama_model_loader: - kv   5:                        gpt2.context_length u32              = 1024\n",
            "llama_model_loader: - kv   6:                      gpt2.embedding_length u32              = 768\n",
            "llama_model_loader: - kv   7:                   gpt2.feed_forward_length u32              = 3072\n",
            "llama_model_loader: - kv   8:                  gpt2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv   9:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  11:                         tokenizer.ggml.pre str              = gpt-2\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,50257]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,50257]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                          general.file_type u32              = 10\n",
            "llama_model_loader: - type  f32:   99 tensors\n",
            "llama_model_loader: - type q2_K:   24 tensors\n",
            "llama_model_loader: - type q3_K:   24 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q2_K - Medium\n",
            "print_info: file size   = 63.66 MiB (4.29 BPW) \n",
            "load: printing all EOG tokens:\n",
            "load:   - 50256 ('<|endoftext|>')\n",
            "load: special tokens cache size = 1\n",
            "load: token to piece cache size = 0.3060 MB\n",
            "print_info: arch             = gpt2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 1024\n",
            "print_info: n_embd           = 768\n",
            "print_info: n_layer          = 12\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 12\n",
            "print_info: n_rot            = 64\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 64\n",
            "print_info: n_embd_head_v    = 64\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 768\n",
            "print_info: n_embd_v_gqa     = 768\n",
            "print_info: f_norm_eps       = 1.0e-05\n",
            "print_info: f_norm_rms_eps   = 0.0e+00\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 3072\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = -1\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 1024\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 0.1B\n",
            "print_info: model params     = 124.44 M\n",
            "print_info: general.name     = Gpt2 Ye Trump\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 50257\n",
            "print_info: n_merges         = 50000\n",
            "print_info: BOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOT token        = 50256 '<|endoftext|>'\n",
            "print_info: UNK token        = 50256 '<|endoftext|>'\n",
            "print_info: PAD token        = 50256 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 50256 '<|endoftext|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:   CPU_REPACK model buffer size =    15.50 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =    62.93 MiB\n",
            "..................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = auto\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) > n_ctx_train (1024) -- possible training context overflow\n",
            "\u001b[0mllama_context:        CPU  output buffer size =     0.19 MiB\n",
            "llama_kv_cache:        CPU KV buffer size =   144.00 MiB\n",
            "llama_kv_cache: size =  144.00 MiB (  4096 cells,  12 layers,  1/1 seqs), K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
            "llama_context: Flash Attention was auto, set to enabled\n",
            "llama_context:        CPU compute buffer size =   101.16 MiB\n",
            "llama_context: graph nodes  = 394\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added <|endoftext|> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "main: llama threadpool init, n_threads = 1\n",
            "main: model was trained on only 1024 context tokens (4096 specified)\n",
            "\u001b[0m\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "sampler seed: 113446202\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 2.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 1.000\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 100, n_keep = 0\n",
            "\n",
            "hole  I love when I first see a**k before sleep, but then again is that shit... and so what's the deal with waking up?”My entire self-conscious. ”Your best friends will tell you to wake your ass. That’s also makes my day go better!***� Thank God, don't get flushing or this stuff ... calm down ❤️\"▶You have no need fessama‘I think I would be the\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =     122.16 ms /   102 runs   (    1.20 ms per token,   834.96 tokens per second)\n",
            "llama_perf_context_print:        load time =     546.79 ms\n",
            "llama_perf_context_print: prompt eval time =      55.03 ms /     2 tokens (   27.52 ms per token,    36.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2116.33 ms /    99 runs   (   21.38 ms per token,    46.78 tokens per second)\n",
            "llama_perf_context_print:       total time =    2802.53 ms /   101 tokens\n",
            "llama_perf_context_print:    graphs reused =         98\n",
            "llama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\n",
            "llama_memory_breakdown_print: |   - Host               |                  308 =    62 +     144 +     101                |\n",
            "llama_memory_breakdown_print: |   - CPU_REPACK         |                   15 =    15 +       0 +       0                |\n"
          ]
        }
      ]
    }
  ]
}